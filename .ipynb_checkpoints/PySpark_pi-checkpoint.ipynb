{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c55058",
   "metadata": {},
   "source": [
    "# What is Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa25662",
   "metadata": {},
   "source": [
    "Spark is a data analytics platform for large data. Spark can work on Hadoop (a distributed file system and computing system).  The image of each spark worker node and spark head node can be seen below. Spark really shines when working with very large datasets (usually GBs or more).  The magic of Spark is its resilient distributed datasets.  Spark can also integrate with pandas, a python library that utilizes dataframes.  In fact it is recommended to use pandas for small datasets and switch to Spark for larger datasets.  We'll show this with an example of calculating pi. \n",
    "\n",
    "Each worker is where the magic happens -- work is done in parallel or distributed depending on how the code is configured. The diagram below shows how Spark is configured on Domino.  We access spark through our Jupyter notebook or through a terminal.  For this tutorial we are using a notebook.  The Spark context command will controll the drive which in turn communicates with a resource manager (usually Yarn, which is part of the Hadoop framework).  Work happens on the executors when using Spark in distributed mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607e918",
   "metadata": {},
   "source": [
    "### Initiate and Test Spark Workers with Pi Calculation\n",
    "\n",
    "Let's do some simple calculations to illustrate why its best to use Spark for large data in dataframes, rather than simple calculations. When we compare using a single function only to calculate spark compared to not using Spark at all we notice something counter-intuitive.  Paralellism has not actually worked -- it slowed down the wall clock time for this calculation.  We're going to show this by doing the following steps:\n",
    "\n",
    "1. Import necessary libraries\n",
    "2. Ensure that the spark context has not been started yet\n",
    "3. Calculate pi without using Spark (na√Øve method)\n",
    "4. Calculate pi using Spark with parallelism\n",
    "5. Stop our spark session (do every time you run a set of calculations or the work will continue indefinitely)\n",
    "\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ad0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76d8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active spark contexts. Ready to create a new one.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"No active spark contexts. Ready to create a new one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c716b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.73 seconds elapsed for naive method and n = 10000000\n",
      "pi is roughly 3.1415992\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from random import random\n",
    "\n",
    "inside = 0\n",
    "n = 10000000\n",
    "\n",
    "t_0 = time()\n",
    "for i in range(n):\n",
    "    x, y = random(), random()\n",
    "    if x**2 + y**2 < 1:\n",
    "        inside += 1\n",
    "print(np.round(time()-t_0, 3), \"seconds elapsed for naive method and n =\", n)\n",
    "print(\"pi is roughly\", inside/n*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342e30cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/domino/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.346 seconds elapsed for spark approach and n= 10000000\n",
      "Pi is roughly 3.142814\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Calculate Pi').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "n = 10000000\n",
    "\n",
    "\n",
    "def inside(p):\n",
    "    # p is useless here\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "\n",
    "t_0 = time()\n",
    "\n",
    "count = sc.parallelize(range(0, n)) \\\n",
    "             .map(inside).reduce(add)\n",
    "print(np.round(time()-t_0, 3), \"seconds elapsed for spark approach and n=\", n)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "# VERY important to stop SparkSession\n",
    "# Otherwise, the job will keep running indefinitely\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8135c",
   "metadata": {},
   "source": [
    "### Calculating Pi using a Distributed Resillent Dataset\n",
    "\n",
    "What happened above?  Why was Spark almost twice the time to calculate pi compared to our method without spark?  This is a trap once can fall into when using Spark.  The way we wrote the code above, Spark is actually making each of its executors make *the same calculation for each of its workers*.  This is because there was nothing to distribute.  So spark tries to execute in parallel by making the calculation more than once.  We are also using a very small set of data (only one number) to calculate pi.  \n",
    "\n",
    "Let's see what happens when we calculate pi using a resillent distributed dataset.  Will Spark be faster than the naive calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a15fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from random import random\n",
    "from operator import add\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def naive_method_time(n):\n",
    "    inside = 0\n",
    "\n",
    "    t_0 = time()\n",
    "    for i in range(n):\n",
    "        x, y = random(), random()\n",
    "        if x**2 + y**2 < 1:\n",
    "            inside += 1\n",
    "    return(np.round(time()-t_0, 3))\n",
    "\n",
    "\n",
    "def spark_method_time(n):\n",
    "    def inside(p):\n",
    "        # p is useless here\n",
    "        x, y = random(), random()\n",
    "        return 1 if x*x + y*y < 1 else 0\n",
    "    t_0 = time()\n",
    "    ### create a RDD dataset\n",
    "    sc.parallelize(range(0, n)) \\\n",
    "        .map(inside).reduce(add)\n",
    "    return(np.round(time()-t_0, 3))\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('Calculate Pi').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "N = [10000, 50000, 100000, 500000,\n",
    "     1000000, 5000000, 10000000, 50000000, 100000000]\n",
    "T = []\n",
    "T_spark = []\n",
    "for n in N:\n",
    "    T_spark.append(spark_method_time(n))\n",
    "    T.append(naive_method_time(n))\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "plt.plot(N, T, label=\"naive\")\n",
    "plt.plot(N, T_spark, label=\"spark\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Total number of points.\")\n",
    "plt.ylabel(\"Time to estimate pi (en sec.)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeacf29",
   "metadata": {},
   "source": [
    "### Spark is Faster\n",
    "\n",
    "When using a resilent distributed dataset in this context, it doesn't matter how much or what data is in there -- right now we are just showing that RDDs are faster than a single number.  With the use of an RDD Spark can actually distribute calculations over many machines (or nodes).  That means each node will perform the calculation in parallel.  Hence we split up the calculations, distributed them, and got a speed improvement of nearly five times the naive function.  For a further look at the example above, see the following page, where much of the code it taken from for Spark's first execution of the pi calculation: [Spark Examples](!https://spark.apache.org/examples.html#:~:text=Pi%20estimation,this%20to%20get%20our%20estimate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470a279",
   "metadata": {},
   "source": [
    "### Other Things to Explore In Spark\n",
    "\n",
    "- mapper functions\n",
    "- rdd datasets (resilient distributed dataset) advanced analytics and machine learning\n",
    "- training across workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc0317",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Spark cheat sheet: [Spark Cheat Sheet](http://datacamp-community-prod.s3.amazonaws.com/02213cb4-b391-4516-adcd-57243ced8eed)\n",
    "2. Spark documentation: [Apache Spark](https://spark.apache.org/docs/latest/)\n",
    "3. Blog Article on Spark: [Article](https://docs.ovh.com/us/en/data-processing/pi-spark/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
