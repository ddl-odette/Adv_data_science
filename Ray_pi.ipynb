{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ray?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for simplifying ML compute.\n",
    "\n",
    "Today’s ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n",
    "\n",
    "Ray is a unified way to scale Python and AI applications from a laptop to a cluster.\n",
    "\n",
    "With Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ray Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the tutorial we showcase what ray can do to speed up code and functions.  We will show how a simple decorator function enables a standard written python function to be run in a parallelized manner and distributed across nodes.\n",
    "\n",
    "The second part of this tutorial focuses on the cart-pole problem. A cart has a pole fixed with a movable lever in the middle of the cart. The cart slides along a frictionless surface. The goal is to keep the pole upright at all times. The test is how far back and forth the cart can move in order to prevent the pole from falling. The tutorial has been modified heavily so that it (i) runs in a jupyter notebook, (ii) demonstrates full capabilities of ray, and ray tune and (iii) breaks down the components of a RL project along with enhanced explainations of the code. We may modify this tutorial further to solve a different problem.\n",
    "\n",
    "In the third part of the tutorial, we demonstrate how to create a custom reinforcement learning environment with the problem space of a robot walking down a corridor.\n",
    "\n",
    "#### References:\n",
    "\n",
    "Barto, A. G., Sutton, R. S. and Anderson, C. (1983), ‘Neuron-like adaptive elements that can solve difficult learning control problems’, IEEE Transactions on Systems, 5, Man, and Cybernetics 13, 834–846\n",
    "\n",
    "Tune: A Research Platform for Distributed Model Selection and Training, Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion, arXiv preprint arXiv:1807.05118}, 2018\n",
    "\n",
    "Ray RLLib Documentation: [Ray RLLib Documentation](https://docs.ray.io/en/latest/rllib-training.html#getting-started)\n",
    "\n",
    "Ray Tune Documentation: [Ray Tune Documentation](https://docs.ray.io/en/latest/tune/index.html)\n",
    "\n",
    "Mastering Reinforcement Learning with Python, Enes Bilgin, Packt Publishing, 2020 [Buy MRL with Python](https://www.amazon.com/Mastering-Reinforcement-Learning-Python-next-generation/dp/1838644148/?tag=meastus-200)\n",
    "\n",
    "Example of Calculating Pi using Ray [How to scale Python multiprocessing to a cluster with one line of code by Evan Oaks](https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Ray Version, Instantiating Ray Instances and Looking at Node Parameters\n",
    "\n",
    "Its typically helpful to check the parameters for nodes to ensure that they are in good shape.  One can also navigate to the tab which says 'Ray Web UI) to look through the node pool and ray actors as well as memory.  These are advanced topics and are meant for trouble-shooting only. \n",
    "\n",
    "\n",
    "In this notebook we'll start with showing you how easy it is to use Ray to convert regular functions into ones that are parallelized and distributed across nodes.  Before we do anything though, let's check our version of Ray.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray, version 1.9.2\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "   service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "   service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "   #_temp_dir='/domino/datasets/local/{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "   ray.util.connect(f\"{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the health of the nodes, look at their CPU and GPU per node.  Here you can see each node, including the head node have seven GPUs (this may differ in your example depending on your environment).  It's a good idea to check this and plan for memory usuage with Ray.  If there isn't enough memory overhead for the code as written, a data channel error will shutdown.  There are advanced techniques to prevent this.  This happens regardless of the verison of Ray used, so make sure to check each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NodeID': '6c1bd5d13c200f8a543af8fd923e5f221d8930a63f326c8a0d790c01',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.122.170',\n",
       "  'NodeManagerHostname': 'ray-633afdf3a2b933058843e442-ray-worker-0',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2022-10-03_08-21-27_764991_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2022-10-03_08-21-27_764991_1/sockets/raylet',\n",
       "  'MetricsExportPort': 62122,\n",
       "  'alive': True,\n",
       "  'Resources': {'memory': 2956748391.0,\n",
       "   'CPU': 1.0,\n",
       "   'node:10.0.122.170': 1.0,\n",
       "   'object_store_memory': 1267177881.0}},\n",
       " {'NodeID': '99a7ad31904635a50ccfce44259e09f0df1de061c24095e1ebaa267d',\n",
       "  'Alive': True,\n",
       "  'NodeManagerAddress': '10.0.96.53',\n",
       "  'NodeManagerHostname': 'ray-633afdf3a2b933058843e442-ray-head-0',\n",
       "  'NodeManagerPort': 2385,\n",
       "  'ObjectManagerPort': 2384,\n",
       "  'ObjectStoreSocketName': '/tmp/ray/session_2022-10-03_08-21-27_764991_1/sockets/plasma_store',\n",
       "  'RayletSocketName': '/tmp/ray/session_2022-10-03_08-21-27_764991_1/sockets/raylet',\n",
       "  'MetricsExportPort': 59168,\n",
       "  'alive': True,\n",
       "  'Resources': {'object_store_memory': 1262284800.0,\n",
       "   'memory': 2524569600.0,\n",
       "   'CPU': 1.0,\n",
       "   'node:10.0.96.53': 1.0}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ray and what can it do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray is a flexible distributed computing system available on Domino product on demand.  With Ray one can run code both in parallel or in distributed mode.  Parallel mode refers to running a function on several threads simultaneously in parallel.  This method can also be accomplished on multiple nodes at once (distributed computing).  One will notice that the wall clock time (which we compute below) differs from the compute time.  With multiple nodes or threading (running in a distributed fashion), the compute time is split among nodes.  Thus when we provide a 10 second 'sleep' we can see that the 10 seconds is distributed and so the wall clock time (the time we actually experience) is shorter than compute time.  This is part of the magic of parallel and distributed computing.  Let's take a closer look below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating remote objects\n",
    "\n",
    "Put an object in Ray's object store, get it out and run the function\n",
    "say want to add 10 million and after every million 5 seconds, total processing would be 50 seconds\n",
    "\n",
    "Do this in ray, and have 3 ray workers, adding 1 million values each, \n",
    "after calculating 1 million each sleeps 5 seconds, then total processing takes less than six seconds\n",
    "iterations in learning \n",
    "ml is already iterative, running partitions on each worker and at the distributed sequentially now paralellized\n",
    "call without ray and then with ray\n",
    "small amount of data, run and then kick off with same code but a larger data set, locally and in cloud testuse 10 workers, each sleeps 2 seconds, and see the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "def sample(num_samples):\n",
    "    num_inside = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if math.hypot(x, y) <= 1:\n",
    "            num_inside += 1\n",
    "    return num_inside\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    start = time.time()\n",
    "    num_inside = sample(num_samples)\n",
    "    \n",
    "    print(\"pi ~= {}\".format((4*num_inside)/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi ~= 3.14158932\n",
      "CPU times: user 1min 5s, sys: 3.84 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "approximate_pi(10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "def sample(num_samples):\n",
    "    num_inside = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if math.hypot(x, y) <= 1:\n",
    "            num_inside += 1\n",
    "    return num_inside\n",
    "\n",
    "def approximate_pi_parallel(num_samples):\n",
    "    from multiprocessing.pool import Pool\n",
    "    pool = Pool()\n",
    "    \n",
    "    start = time.time()\n",
    "    num_inside = 0\n",
    "    sample_batch_size = 100000\n",
    "    for result in pool.map(sample, [sample_batch_size for _ in range(num_samples//sample_batch_size)]):\n",
    "        num_inside += result\n",
    "        \n",
    "    print(\"pi ~= {}\".format((4*num_inside)/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi ~= 3.14185292\n",
      "CPU times: user 276 ms, sys: 60.5 ms, total: 337 ms\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "approximate_pi_parallel(10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "def sample(num_samples):\n",
    "    num_inside = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if math.hypot(x, y) <= 1:\n",
    "            num_inside += 1\n",
    "    return num_inside\n",
    "\n",
    "def approximate_pi_distributed(num_samples):\n",
    "    from ray.util.multiprocessing.pool import Pool # NOTE: Only the import statement is changed.\n",
    "    pool = Pool()\n",
    "        \n",
    "    start = time.time()\n",
    "    num_inside = 0\n",
    "    sample_batch_size = 100000\n",
    "    for result in pool.map(sample, [sample_batch_size for _ in range(num_samples//sample_batch_size)]):\n",
    "        num_inside += result\n",
    "        \n",
    "    print(\"pi ~= {}\".format((4*num_inside)/num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi ~= 3.14152188\n",
      "CPU times: user 119 ms, sys: 20.4 ms, total: 140 ms\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "approximate_pi_distributed(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above examples the compute time differs and the wall clock time for the compute differs. However keep in mind Ray is only using three workers to calculate pi in this simple example becasue we started the cluster with three workers.  If the cluster is started with more workers, it will speed up the calculations every further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note about Warnings\n",
    "\n",
    "Version 1.6 is validated on the latest edition of Domino, but you see here we chose to use the latest stable version of Ray, 1.9.  This version will occasionally throw some warning about depreciation for future versions of Ray or Pytorch because it is the newest stable version of Ray.  Don't worry too much about the warnings, they will not change the procedures followed to run the code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
