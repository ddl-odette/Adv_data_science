{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972a3ea0",
   "metadata": {},
   "source": [
    "### Absolute Beginners Tutorial for MPI and DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c24d3",
   "metadata": {},
   "source": [
    "Today we'll review an absolute beginner's use of MPI and DeepSpeed.  For this tutorial we'll use OpenMPI which is a message passing interface often used at super-computing centers for high performance computing.  It can be used for problems that require parallelism or those that require distributed computing (where processes require careful coordination).  OpenMPI on Domino Data Lab is easy to use.  \n",
    "\n",
    "Here are a few tips on hardware choices:\n",
    "\n",
    "1. Make sure to choose the correct hardware.  If working with small to medium size data (under a gigabyte) often a small or medium hardware tier is sufficient.\n",
    "2. If using large datasets (larger than a GB) it is often useful to select a high memory hardware tier in either a CPU or GPU.\n",
    "3. If using a complex calculation with very large data (larger than a few GB) then hardware \n",
    "\n",
    "For DeepSpeed use cases, the models and / or data is typically large, so a high-memory, GPU is most helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face2e0c",
   "metadata": {},
   "source": [
    "### What is OpenMPI?\n",
    "\n",
    "OpenMPI is a message passing interface library that is open-sourced.  Its a popular choice for super-computing centers.  The Open MPI Project is an MPI implementat that is created and maintained by a group of academic, research and industry partners.  The High Performance Computing community contributes the best code and technologies to the OpenMPI library.  Some of the features of OpenMPI include:\n",
    "\n",
    "- Conforms to MPI-3.1 standards\n",
    "- Thread management and concurrency (your computer will not get 'fried')\n",
    "- Support of all networks\n",
    "- Supports most job schedulers\n",
    "- High Performance on all platforms\n",
    "- Open source license (BSD license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ec63f",
   "metadata": {},
   "source": [
    "### How MPI is used in High Performance Computing\n",
    "\n",
    "High Performance Computing (HPC) is simply -- at high speeds -- performing compex calculations.  It run both parallel and distributed problems.  The best known example of HPC is a supercomputer.  Supercomputers are made up of many CPUs or GPUs and processors working together to perform both parallel and distributed tasks.  MPI aids in this by coorindating messaging among CPUs, GPUs and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d2914",
   "metadata": {},
   "source": [
    "### Tutorial and Objectives\n",
    "\n",
    "Upon completion of part one of this tutorial the learner will be able to:\n",
    "    \n",
    "- Describe OpenMPI and its use cases\n",
    "- Identify appropriate hardware\n",
    "- Perform a distributed calculation of pi using MPI workers\n",
    "- Understand use of python files to run programs on a MPI-managed cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da5921",
   "metadata": {},
   "source": [
    "#### A 'Hello World' Example and Calcuate Pi\n",
    "\n",
    "In this example we will use a naïve method (without distributed compute or parallelism) to calculate pi. We will compare this to using OpenMPI to Calculate Pi using a python file, but running the code on a set of workers.  First let's calculate pi the standard way.  Nota Bene: make sure to sync your MPI cluster before running new code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10baf8bb",
   "metadata": {},
   "source": [
    "In order to run a program or python files using MPI and python the code in the file needs to be set-up to run on the cluster.  In this exercise we will look at the number of processes running and the rank for each.  This ``hello world`` example is run from a sepparate python file.  One can run many python programs in the same manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4baf38",
   "metadata": {},
   "source": [
    "```\n",
    "from __future__ import print_function\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "print(\"Hello! I'm rank %d from %d running in total...\" % (comm.rank, comm.size))\n",
    "comm.Barrier()```\n",
    "\n",
    "Notice in the code above we set up the communicator, which indicates the 'world' size or number of cpus or gpus in the cluster on which the program will run.  The ```com.Barrier()``` function tells the program to wait until all processes and workers are synced.  Once that happens the program can be run at the command line using ``mpirun`` along with the appropriate options.  Try this for yourself changing the number of processes.  You can also add or change the 'hello world' code to run a different python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ab08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc79fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpi4py.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42dd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm rank 0 from 3 running in total...\n",
      "Hello! I'm rank 1 from 3 running in total...\n",
      "Hello! I'm rank 2 from 3 running in total...\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 3  python hello_world.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3a04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm rank 1 from 3 running in total...\n",
      "Hello! I'm rank 0 from 3 running in total...\n",
      "Hello! I'm rank 2 from 3 running in total...\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --hostfile /domino/mpi/hosts -np 3 python hello_world.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f94f5f",
   "metadata": {},
   "source": [
    "Calculating pi on 2 workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0bf0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi ~= 3.14146568\n",
      "total time in seconds  77.99676060676575\n",
      "pi ~= 3.14164336\n",
      "total time in seconds  78.48352980613708\n",
      "pi ~= 3.14160336\n",
      "total time in seconds  79.8351399898529\n",
      "CPU times: user 853 ms, sys: 226 ms, total: 1.08 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 3 --hostfile /domino/mpi/hosts --bind-to core --map-by slot python calculate_pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373310f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpirun (Open MPI) 4.1.2a1\n",
      "\n",
      "Usage: mpirun [OPTION]...  [PROGRAM]...\n",
      "Start the given program using Open RTE\n",
      "\n",
      "-c|-np|--np <arg0>       Number of processes to run\n",
      "-h|--help <arg0>         This help message\n",
      "   -n|--n <arg0>         Number of processes to run\n",
      "-q|--quiet               Suppress helpful messages\n",
      "-v|--verbose             Be verbose\n",
      "-V|--version             Print version and exit\n",
      "\n",
      "For additional mpirun arguments, run 'mpirun --help <category>'\n",
      "\n",
      "The following categories exist: general (Defaults to this option), debug,\n",
      "    output, input, mapping, ranking, binding, devel (arguments useful to OMPI\n",
      "    Developers), compatibility (arguments supported for backwards compatibility),\n",
      "    launch (arguments to modify launch options), and dvm (Distributed Virtual\n",
      "    Machine arguments).\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "### how to reach the mpi help file\n",
    "\n",
    "!mpirun --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a25643f",
   "metadata": {},
   "source": [
    "#### Calculating Pi with and without Distributed Compute\n",
    "\n",
    "Below let's look at an example of how to calculate pi without mpi (naïve method) and with mpi (distributed method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c5bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create function for calculation\n",
    "import time\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "def sample(num_samples):\n",
    "    num_inside = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if math.hypot(x, y) <= 1:\n",
    "            num_inside += 1\n",
    "    return num_inside\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    start = time.time()\n",
    "    num_inside = sample(num_samples)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"pi ~= {}\".format((4*num_inside)/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918bc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi ~= 3.1414826\n",
      "CPU times: user 1min 3s, sys: 0 ns, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Perform calculation on 100 million data points\n",
    "\n",
    "approximate_pi(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fb3a7",
   "metadata": {},
   "source": [
    "Now let's compare that calculation to calculating pi using OpenMPI.  For this function we will use a python wrapper / library around MPI called py4mpi.  We will keep our calculation code in a sepparate file so we can run the function over MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2664970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many GPU or CPU workers do you have? 3\n"
     ]
    }
   ],
   "source": [
    "## prepare for distributed calculation using MPI\n",
    "\n",
    "maxprocs = input('How many GPU or CPU workers do you have?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1735ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926535901537\n",
      "CPU times: user 17 s, sys: 7.33 s, total: 24.4 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "comm = MPI.COMM_SELF.Spawn(sys.executable,\n",
    "                           args=['cpi.py'],\n",
    "                           maxprocs=int(maxprocs))\n",
    "\n",
    "N = numpy.array(10**8, 'i')\n",
    "comm.Bcast([N, MPI.INT], root=MPI.ROOT)\n",
    "PI = numpy.array(0.0, 'd')\n",
    "comm.Reduce(None, [PI, MPI.DOUBLE],\n",
    "            op=MPI.SUM, root=MPI.ROOT)\n",
    "print(PI)\n",
    "\n",
    "comm.Disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41419939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.36 ms, sys: 129 µs, total: 9.48 ms\n",
      "Wall time: 671 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 3 --hostfile /domino/mpi/hosts python cpi2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"cpi2.py\", line 4, in <module>\n",
      "    import cupy as cp\n",
      "ModuleNotFoundError: No module named 'cupy'\n",
      "Traceback (most recent call last):\n",
      "  File \"cpi2.py\", line 4, in <module>\n",
      "    import cupy as cp\n",
      "ModuleNotFoundError: No module named 'cupy'\n",
      "Traceback (most recent call last):\n",
      "  File \"cpi2.py\", line 4, in <module>\n",
      "    import cupy as cp\n",
      "ModuleNotFoundError: No module named 'cupy'\n",
      "--------------------------------------------------------------------------\n",
      "Child job 3 terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "(null) detected that one or more processes exited with non-zero status, thus causing\n",
      "the job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[56347,3],1]\n",
      "  Exit code:    1\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comm = MPI.COMM_SELF.Spawn(sys.executable,\n",
    "                           args=['cpi2.py'],\n",
    "                           maxprocs=int(maxprocs))\n",
    "\n",
    "N = numpy.array(10**8, 'i')\n",
    "comm.Bcast([N, MPI.INT], root=MPI.ROOT)\n",
    "PI = numpy.array(0.0, 'd')\n",
    "comm.Reduce(None, [PI, MPI.DOUBLE],\n",
    "            op=MPI.SUM, root=MPI.ROOT)\n",
    "print(PI)\n",
    "\n",
    "comm.Disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824bec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
